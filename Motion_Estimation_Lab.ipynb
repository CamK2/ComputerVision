{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamK2/ComputerVision/blob/main/Motion_Estimation_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:** - how to detect/understand object motion from a series of images (i.e. a video)\n",
        "<img src=\"https://nanonets.com/blog/content/images/2019/04/intro-1-2.gif\">\n",
        "\n",
        "* How can we more specifically pose the question?  \n",
        "* What should be acceptable inputs?   \n",
        "* How do we report a solution (what are outputs)?  \n",
        "  * A grid of motion estimates (sparse optical flow).\n",
        "  * A motion estimate for each pixel (dense optical flow).\n",
        "\n",
        "<img src=\"https://nanonets.com/blog/content/images/2019/04/sparse-vs-dense.gif\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This notebook was based on these articles:[one](https://nanonets.com/blog/optical-flow/), [two](https://androidkt.com/how-to-capture-and-play-video-in-google-colab/)"
      ],
      "metadata": {
        "id": "38WqraYYobj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two cells below allow you to capture a video via Google colab (similar to how we used Code Snippets for taking pictures in the past). Take a short video with some type of motion - Ex: move your hand or head."
      ],
      "metadata": {
        "id": "OaFm95lFru84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript,HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        " \n",
        "def record_video(filename):\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "       \n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        " \n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        " \n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        " \n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
        "     \n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        " \n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        " \n",
        "      await video.play();\n",
        " \n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        " \n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        " \n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "       \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        " \n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(filename,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{filename}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "metadata": {
        "id": "hrYpD4kpq6Z3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"test.mp4\"\n",
        "record_video(video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MIWkzDgfq6Z9",
        "outputId": "906ed564-e6a0-4e92-991e-31de42d9d4dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function recordVideo() {\n",
              "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      const stopCapture = document.createElement(\"button\");\n",
              "       \n",
              "      capture.textContent = \"Start Recording\";\n",
              "      capture.style.background = \"orange\";\n",
              "      capture.style.color = \"white\";\n",
              " \n",
              "      stopCapture.textContent = \"Stop Recording\";\n",
              "      stopCapture.style.background = \"red\";\n",
              "      stopCapture.style.color = \"white\";\n",
              "      div.appendChild(capture);\n",
              " \n",
              "      const video = document.createElement('video');\n",
              "      const recordingVid = document.createElement(\"video\");\n",
              "      video.style.display = 'block';\n",
              " \n",
              "      const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
              "     \n",
              "      let recorder = new MediaRecorder(stream, options);\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              " \n",
              "      video.srcObject = stream;\n",
              "      video.muted = true;\n",
              " \n",
              "      await video.play();\n",
              " \n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              " \n",
              "      await new Promise((resolve) => {\n",
              "        capture.onclick = resolve;\n",
              "      });\n",
              "      recorder.start();\n",
              "      capture.replaceWith(stopCapture);\n",
              " \n",
              "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
              "      recorder.stop();\n",
              "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
              "      let arrBuff = await recData.data.arrayBuffer();\n",
              "       \n",
              "      // stop the stream and remove the video element\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              " \n",
              "      let binaryString = \"\";\n",
              "      let bytes = new Uint8Array(arrBuff);\n",
              "      bytes.forEach((byte) => {\n",
              "        binaryString += String.fromCharCode(byte);\n",
              "      })\n",
              "    return btoa(binaryString);\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished recording video at:test.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "\n",
        "\n",
        "# corner features\n",
        "feature_params = dict(maxCorners=300,\n",
        "                      qualityLevel = 0.2,\n",
        "                      minDistance = 2,\n",
        "                      blockSize = 7)\n",
        "lk_params = dict(winSize=(15,15),\n",
        "                 maxLevel=2,\n",
        "                 criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT,\n",
        "                           10,0.03))\n",
        "# input our video file\n",
        "cap = cv.VideoCapture(\"test.mp4\")\n",
        "writer = cv.VideoWriter(\"output.avi\",\n",
        "                        cv.VideoWriter_fourcc(*'MPEG'),\n",
        "                        30, (1080, 1920))\n",
        "\n",
        "# specify color for optical flow line, pink\n",
        "color = (255,int(0.7*255),int(0.7*255))\n",
        "\n",
        "# grab first frame\n",
        "ret, first_frame = cap.read()\n",
        "# grayscale\n",
        "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
        "# find corners\n",
        "prev = cv.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
        "# create a mask\n",
        "mask = np.zeros_like(first_frame)\n",
        "# loop over video frames\n",
        "while cap.isOpened(): # while video file not empty, keep running\n",
        "  # read next frame\n",
        "  ret, frame = cap.read()\n",
        "  gray = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
        "  # grab corners\n",
        "  prev = cv.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
        "  # calculate the sparse optical flow\n",
        "  next, status, error = cv.calcOpticalFlowPyrLK(prev_gray,\n",
        "                                                gray, prev, None,\n",
        "                                                **lk_params)\n",
        "  # pick only good corner feature matches\n",
        "  good_old = prev[status==1].astype(int)\n",
        "  good_new = next[status==1].astype(int)\n",
        "\n",
        "  # draw the optical flow lines\n",
        "  for i, (new,old) in enumerate(zip(good_new, good_old)):\n",
        "    a,b = new.ravel()\n",
        "    c,d = old.ravel()\n",
        "\n",
        "    mask = cv.line(mask, (a,b), (c,d), color, 2)\n",
        "    frame = cv.circle(frame, (a,b), 3, color, -1)\n",
        "  # overlay all lines onto current frame\n",
        "  output = cv.add(frame, mask) # create a new \"frame\" with both\n",
        "\n",
        "  # update before next loop iteration\n",
        "  prev_gray = gray.copy() # deep copy\n",
        "  prev = good_new.reshape(-1,1,2) # update good feature point\n",
        "  writer.write(output)\n",
        "\n",
        "  # break code\n",
        "  if output.any() or (cv.waitKey(10) & 0xFF == ord('q')):\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "writer.release()"
      ],
      "metadata": {
        "id": "TQA5COVZNdce"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "Grayscale the video."
      ],
      "metadata": {
        "id": "GO7dQcsertbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optical Flow steps\n",
        "\n",
        "* Put video in a \"good\" format\n",
        "* Identify/specify a set of (x,y) features to track.\n",
        "* Find the displacement of each feature between successive frames:\n",
        "  * Do a convolution/kernel \"scan\" of the feature pixels in each subsequent image.\n",
        "  * Calculate the x and y displacement and record it for each feature. "
      ],
      "metadata": {
        "id": "jdIdmpWOtftt"
      }
    }
  ]
}